{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import qmc\n",
    "import numpy as np\n",
    "from globalVar import dt\n",
    "from scipy import optimize\n",
    "\n",
    "# Batch design parameters\n",
    "sites = 2**9; batch = 20; nsim = sites * batch\n",
    "k = 8\n",
    "# parameters \n",
    "\n",
    "    \n",
    "maturity = 48 # units are in hours\n",
    "global dt; dt = 15/60;\n",
    "nstep = int(maturity/dt)\n",
    "# B and I range\n",
    "B_min = -2.5\n",
    "B_max = 2.5\n",
    "I_max = 10\n",
    "# OU params\n",
    "alpha = 0.5; m0 = 0; sigma = 1\n",
    "\n",
    "bounds  = sigma**2 *(1- np.exp(-2* alpha *48))/(2*alpha)\n",
    "\n",
    "# uniform is bad, sample from Xt directly, \n",
    "\n",
    "\n",
    "def create_samples():\n",
    "    # samplings\n",
    "    sampler = qmc.Sobol(d=2, scramble=False)\n",
    "    # 1024 samples : simple spacde fillling designs\n",
    "    W = sampler.random_base2(m=k)\n",
    "    l_bounds = [-3, 0]\n",
    "    u_bounds = [3, 10]\n",
    "    W= qmc.scale(W, l_bounds, u_bounds)\n",
    "    global X_prev1; X_prev1 = W[:,0]\n",
    "    #print(np.min(X_prev1),np.max(X_prev1))\n",
    "    global I_next1; I_next1 = W[:,1];    # Ic in[0,Imax = 10]\n",
    "\n",
    "    sampler = qmc.Sobol(d=2, scramble=False)\n",
    "    # 1024 samples : simple spacde fillling designs\n",
    "    W = sampler.random_base2(m=k)\n",
    "    l_bounds = [-3, 0]\n",
    "    u_bounds = [3, 3]\n",
    "    W= qmc.scale(W, l_bounds, u_bounds)\n",
    "    global X_prev2; X_prev2= W[:,0]\n",
    "    global I_next2; I_next2 = W[:,1];    # Ic in[0,Imax = 10]\n",
    "    #print(np.min(I_next2),np.max(I_next2))\n",
    "    sampler = qmc.Sobol(d=2, scramble=False)\n",
    "    # 1024 samples : simple spacde fillling designs\n",
    "    W = sampler.random_base2(m=k)\n",
    "    l_bounds = [-3, 7]\n",
    "    u_bounds = [3, 10]\n",
    "    W= qmc.scale(W, l_bounds, u_bounds)\n",
    "    global X_prev3; X_prev3 = W[:,0]\n",
    "    global I_next3; I_next3 = W[:,1];    # Ic in[0,Imax = 10]\n",
    "    X_prev = np.concatenate((X_prev1,X_prev2,X_prev3))\n",
    "    I_next = np.concatenate((I_next1,I_next2,I_next3))\n",
    "    return X_prev,I_next\n",
    "\n",
    "t = np.arange(0,maturity+dt,dt)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def demandSimulate(α, m, σ, n_step, n_sim, maturity, P0,given_W = False, W = None):\n",
    "    \n",
    "    dt = maturity/n_step\n",
    "    priceMatrix = np.zeros((n_sim, n_step+1))\n",
    "    priceMatrix[:,0] = np.ones(n_sim) * P0\n",
    "    \n",
    "    if not given_W:\n",
    "        dW = np.random.normal(0,1,size = (n_sim,n_step) ) * np.sqrt(dt)\n",
    "    \n",
    "    else:\n",
    "        dW = W\n",
    "    \n",
    "    \n",
    "    for i in range(1,n_step+1):\n",
    "        priceMatrix[:,i] = priceMatrix[:,i-1] + α * (m - priceMatrix[:,i-1]) * dt + σ * dW[:,i-1]\n",
    "        \n",
    "    return priceMatrix\n",
    "\n",
    "\n",
    "def finalCost(I):\n",
    "    return 200*np.maximum(5-I,0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4-layer NN, 1-dim input layer, 2 hidden layers with 12-dim and 1 output layer with 1-dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2,64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_objective(B,X,I,mdl):\n",
    "    next_step = np.array([X,I +B[0]*dt])\n",
    "    if mdl == finalCost:\n",
    "        objective_func = np.abs((X+B[0])**2) * dt  + mdl(next_step[1] )\n",
    "    else:\n",
    "        objective_func = np.abs((X+B[0])**2) * dt + float(mdl(torch.Tensor(next_step))[0])\n",
    "\n",
    "\n",
    "    return objective_func\n",
    "\n",
    "# derivative for final cost not implemented\n",
    "def one_step_derivative(B,X,I,mdl):\n",
    "    if mdl == finalCost:\n",
    "        return None\n",
    "\n",
    "    next_step = np.array([X,I +B[0]*dt])\n",
    "    next_step = torch.Tensor(next_step)\n",
    "    next_step.requires_grad_(True)\n",
    "    pred = mdl(next_step)\n",
    "    grad = float(torch.autograd.grad(pred.sum(),next_step)[0][1])\n",
    "    objective_der = 2 * (X+B[0]) * dt + grad * dt\n",
    "    return objective_der\n",
    "\n",
    "def minimize(args):\n",
    "    f,grad_f,x,i,mdl,lb,ub = args\n",
    "    if mdl == finalCost:\n",
    "        grad_f = None\n",
    "    bnds = optimize.Bounds(lb, ub )\n",
    "    starter = np.maximum(lb,np.minimum(-x,ub))\n",
    "    res = optimize.minimize(f, jac= grad_f,x0=starter, args=(x,i,mdl),method = \"L-BFGS-B\", bounds = bnds)\n",
    "    return res.x[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create (X_{T-1},I_T)\n",
    "X_prev,I_next = create_samples()\n",
    "# create conditional expectation models to save \n",
    "global Model ; Model = [None] * (nstep)\n",
    "NN_Mdl = None\n",
    "# init warm-starters for emulators\n",
    "V_opt_start = None\n",
    "# this computes E[V(X_T)|I_(T-1)], no regression \n",
    "Model[nstep-1] = finalCost\n",
    "\n",
    "# assume we generated X_T-2,I_(T-1)\n",
    "X_prev,I_next = create_samples()\n",
    "# create X_(T-2)->X_(T-1)\n",
    "demandMatrix = demandSimulate(alpha, m0, sigma, 1, len(X_prev), dt, X_prev);\n",
    "\n",
    "# finding controls at T-1\n",
    "optimal_B = np.zeros(len(X_prev))\n",
    "\n",
    "LB = np.maximum(B_min, (-I_next)/dt)\n",
    "UB = np.minimum(B_max, (I_max-I_next)/dt)\n",
    "sample_num = len(X_prev)\n",
    "# not using derivative is better and no difference\n",
    "args =[(one_step_objective,None,demandMatrix[i,1],I_next[i],finalCost,LB[i],UB[i]) for i in range(sample_num)]\n",
    "#p = Pool()\n",
    "#optimal_B = np.array(list(p.imap(minimize,args)))\n",
    "#print(optimal_B)\n",
    "\n",
    "idx = 0\n",
    "for arg in args:\n",
    "    optimal_B[idx] = minimize(arg)\n",
    "    idx+=1\n",
    "# cost(T-1)|X(T-2) = (B(T-1)+X(T-1)**2)*dt + E[X_T|I_(T-1)]\n",
    "NN_test = np.column_stack((demandMatrix[:,1], I_next + optimal_B *dt))\n",
    "costNext = np.abs((demandMatrix[:,1]+optimal_B)**2) *dt + finalCost(NN_test[:,1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "191\n",
      "Epoch: 0\n",
      "Loss: 37312.332031\n",
      "Epoch: 1000\n",
      "Loss: 30899.296875\n",
      "Epoch: 2000\n",
      "Loss: 30899.298828\n",
      "Epoch: 3000\n",
      "Loss: 30899.296875\n",
      "Epoch: 4000\n",
      "Loss: 30899.296875\n",
      "Epoch: 5000\n",
      "Loss: 30899.296875\n",
      "Epoch: 6000\n",
      "Loss: 30899.296875\n",
      "Epoch: 7000\n",
      "Loss: 30899.296875\n",
      "Epoch: 8000\n",
      "Loss: 30899.296875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m Loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 25\u001b[0m opt\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m n\u001b[39m%\u001b[39m\u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(n))\n",
      "File \u001b[0;32m~/miniconda3/envs/energy_mike/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/energy_mike/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/energy_mike/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/energy_mike/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/energy_mike/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iStep in range(nstep-1,0,-1):\n",
    "    print(\"\\n\")\n",
    "    print(iStep)\n",
    "    # do regression to learn  cost(t)|X(t-1) as function of X(t-1) and I(t) where t = T-1,T-2,...,1\n",
    "    X_train = torch.Tensor(np.column_stack((X_prev, I_next)))\n",
    "    y_train = torch.Tensor(costNext)\n",
    "\n",
    "    NN_Mdl = NN()\n",
    "    param_list = NN_Mdl.parameters()\n",
    "    # Set up the optimizer (Adam, can also replace with SGD)\n",
    "    eta = 0.05\n",
    "    opt = torch.optim.Adam(param_list,lr = eta)\n",
    "\n",
    "    N_epoch =20000\n",
    "    for n in range(0,N_epoch):    \n",
    "\n",
    "        \n",
    "        est_costNext = torch.squeeze(NN_Mdl(X_train))\n",
    "\n",
    "\n",
    "        total_loss = (est_costNext - y_train)**2\n",
    "        Loss = torch.mean(total_loss)\n",
    "        opt.zero_grad()\n",
    "        Loss.backward()\n",
    "        opt.step()\n",
    "        if n%1000 == 0:\n",
    "            print(\"Epoch: \" + str(n))\n",
    "            print(\"Loss: \" + str(round(Loss.detach().item(),6)))\n",
    "\n",
    "    Model[iStep-1] = NN_Mdl\n",
    "    # assuming we generated, X_t-2,I_t-1 \n",
    "    X_prev,I_next = create_samples()\n",
    "    # create path from X_t-2 to X_t-1,\n",
    "    demandMatrix = demandSimulate(alpha, m0, sigma, 1, len(X_prev), dt, X_prev);\n",
    "    # finding controls at t-1\n",
    "    optimal_B = np.zeros(len(X_prev))\n",
    "    LB = np.maximum(B_min, (-I_next)/dt)\n",
    "    UB = np.minimum(B_max, (I_max-I_next)/dt)\n",
    "    sample_num = len(X_prev)\n",
    "    args =[(one_step_objective,one_step_derivative,demandMatrix[i,1],I_next[i],NN_Mdl,LB[i],UB[i]) for i in range(sample_num)]\n",
    "    #p = Pool()\n",
    "    #optimal_B = np.array(list(p.imap(minimize,args)))\n",
    "    idx = 0\n",
    "    for arg in args:\n",
    "        optimal_B[idx] = minimize(arg)\n",
    "        idx+=1\n",
    "\n",
    "\n",
    "    NN_test = np.column_stack((demandMatrix[:,1], I_next + optimal_B *dt))\n",
    "\n",
    "    # learn cost(t-1)|x(t-2)\n",
    "    costNext = np.abs((demandMatrix[:,1]+optimal_B)**2) *dt + torch.squeeze(NN_Mdl(torch.Tensor(NN_test))).detach().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from globalVar import dt\n",
    "from scipy import optimize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy_mike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
